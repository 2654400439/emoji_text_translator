from preprocess.load_data import load_test, load_train, load_emoji_4_columns
import emoji
from collections import Counter
from tqdm import trange
from nltk.lm import Lidstone
from nltk.lm.preprocessing import padded_everygram_pipeline


def generate_handle_list():
    """
    ç”Ÿæˆéœ€è¦ä½¿ç”¨ngramå¤„ç†çš„emojiåˆ—è¡¨
    :return: emojiåˆ—è¡¨
    """
    # åŠ è½½éœ€è¦çš„æ•°æ®
    data_test = load_test()
    data_emoji = load_emoji_4_columns()
    data_emoji_key = []
    for i in range(len(data_emoji)):
        data_emoji_key.append(data_emoji[i][0])

    tmp = []
    for i in range(len(data_test)):
        sample_emoji = data_test[i][0].split('\t')[1]
        for j in range(len(emoji.emoji_list(sample_emoji))):
            tmp.append(emoji.emoji_list(sample_emoji)[j]['emoji'])
    # é€šè¿‡åœ¨æµ‹è¯•é›†ä¸­ç»Ÿè®¡emojiå‡ºç°æƒ…å†µï¼Œè¿›è¡Œè¿‡æ»¤
    Counter_tmp = Counter(tmp)
    tmp = dict(Counter_tmp)

    # æµ‹è¯•é›†é‡Œæ‰€æœ‰å‡ºç°æ¬¡æ•°å¤§äº10çš„emoji
    handle_list = []
    for j in range(len(list(tmp.keys()))):
        if list(tmp.keys())[j] in data_emoji_key:
            if len(data_emoji[data_emoji_key.index(list(tmp.keys())[j])][1].split(',')) > 1:
                if tmp[list(tmp.keys())[j]] >= 10:
                    handle_list.append(list(tmp.keys())[j])

    # è¿‡æ»¤emojiå€™é€‰ç¿»è¯‘å‡ºç°é›†ä¸­ï¼ˆä¸éœ€è¦ä½¿ç”¨ngramï¼‰çš„æƒ…å†µ
    handle_list.remove('ğŸ˜“')
    handle_list.remove('ğŸ”«')
    handle_list.remove('ğŸ¬')
    handle_list.remove('ğŸ‚')
    handle_list.remove('ğŸ‘“')
    handle_list.remove('ğŸ¤¡')
    handle_list.remove('ğŸš‰')
    handle_list.remove('ğŸ©')
    handle_list.remove('ğŸ¥')
    handle_list.remove('ğŸ§')
    handle_list.remove('ğŸš')
    handle_list.remove('ğŸ‘¸ğŸ»')
    handle_list.remove('ğŸ†š')
    handle_list.remove('ğŸˆµ')
    handle_list.remove('ğŸ©²')
    handle_list.remove('ğŸ’µ')
    handle_list.remove('ğŸ‘‹')
    handle_list.remove('ğŸ‘')
    handle_list.remove('ğŸ‘Œ')
    handle_list.remove('ğŸ…')
    handle_list.remove('ğŸ‘‡')
    handle_list.remove('ğŸŒº')
    handle_list.remove('ğŸ˜£')
    handle_list.remove('ğŸš¶')
    handle_list.remove('ğŸ•³')
    handle_list.remove('ğŸ˜ ')
    handle_list.remove('ğŸ’§')
    handle_list.remove('ğŸš„')
    # åŠ ä¸ŠğŸ§„ã€ğŸ“¡ã€ğŸ©ã€ğŸ™ŒğŸ»ã€ğŸ§ªã€ğŸ˜€ã€ğŸ˜‰ã€ğŸ¦„ã€ğŸ“ã€ğŸš­ã€ğŸ‘¨â€ğŸ¦²ã€ğŸ”‹
    handle_list.append('ğŸ§„')
    handle_list.append('ğŸ“¡')
    handle_list.append('ğŸ©')
    handle_list.append('ğŸ™ŒğŸ»')
    handle_list.append('ğŸ§ª')
    handle_list.append('ğŸ˜€')
    handle_list.append('ğŸ˜‰')
    handle_list.append('ğŸ¦„')
    handle_list.append('ğŸ“')
    handle_list.append('ğŸš­')
    handle_list.append('ğŸ”‹')
    handle_list.append('ğŸ‘¨â€ğŸ¦²')
    handle_list.append('ğŸ—¡')
    handle_list.append('ğŸ¥')
    # å¢åŠ çš„å°è¯•0926
    handle_list.append('ğŸ‘´ğŸ»')

    return handle_list


def generate_ngram_data(special_emoji):
    """
    ç”Ÿæˆå¯¹äºæŸä¸ªemojiè®­ç»ƒngramæ¨¡å‹éœ€è¦çš„æ•°æ®
    :param special_emoji: æŒ‡å®šçš„æŸä¸ªemoji
    :return: å¯¹åº”çš„è®­ç»ƒé›†æ•°æ®
    """
    data_train = load_train()

    data_ngram_data = []
    for i in range(len(data_train)):
        sample_emoji = data_train[i][2]
        if len(emoji.emoji_list(sample_emoji)) == 1:
            if emoji.emoji_list(sample_emoji)[0]['emoji'] == special_emoji:
                data_ngram_data.append([data_train[i][2], data_train[i][5]])

    # é’ˆå¯¹åªå ä¸€ä½çš„æ ‡å‡†emojiç”Ÿæˆä¾›è¯­è¨€æ¨¡å‹è®­ç»ƒçš„æ•°æ®é›†ï¼Œé™¤emojiéƒ¨åˆ†ä¸ºå…¶ä½™æŒ‰å­—åˆ†å‰²
    train = []
    for j in range(len(data_ngram_data)):
        sample_emoji = data_ngram_data[j][0]
        sample_new = sample_emoji.split(special_emoji)
        translate = data_ngram_data[j][1][emoji.emoji_list(sample_emoji)[0]['match_start']:data_ngram_data[j][1].find(sample_new[1])]
        left, right = [], []
        for i in range(len(sample_new[0])):
            left.append(sample_new[0][i])
        for i in range(len(sample_new[1])):
            right.append(sample_new[1][i])
        sentence = left + [translate] + right
        train.append(sentence)

    return train


def generate_ngram(special_emoji):
    """
    ç”Ÿæˆ3gramè¯­è¨€æ¨¡å‹ï¼Œä½¿ç”¨æ•°æ®å¹³æ»‘çš„æ–¹æ³•
    :param special_emoji: ç‰¹å®šçš„éœ€è¦ç”Ÿæˆngramçš„emoji
    :return: è¯¥emojiçš„åŸºäºè®­ç»ƒé›†çš„3gramè¯­è¨€æ¨¡å‹
    """
    train_d = generate_ngram_data(special_emoji)

    ngram_order = 3

    train_data, vocab_data = padded_everygram_pipeline(ngram_order, train_d)

    lm = Lidstone(0.2, ngram_order)
    lm.fit(train_data, vocab_data)
    return lm


def generate_lm_list(handle_list):
    """
    ç”Ÿæˆhandle_listä¸­æ‰€æœ‰emojiçš„3gramè¯­è¨€æ¨¡å‹
    :param handle_list: å¾…å¤„ç†emojiåˆ—è¡¨
    :return: è¯­è¨€æ¨¡å‹åˆ—è¡¨
    """
    lm_list = []
    print('--------------------generate_lm_list--------------------')
    for i in trange(len(handle_list)):
        lm_list.append(generate_ngram(handle_list[i]))

    return lm_list


if __name__ == '__main__':
    handle_list = generate_handle_list()
    print(handle_list[0])
    lm_list = generate_lm_list(handle_list)
