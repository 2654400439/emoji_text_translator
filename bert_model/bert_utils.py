from preprocess.load_data import generate_emoji_candidate, load_test
from bert_model.bert import initialize_bert
import emoji
from transformers import pipeline, logging
import torch
from torch import nn
import numpy as np
import difflib
import jieba
from jieba import posseg


def bert_fill_mask(sample_emoji, fill_mask):
    """
    BERTæ©ç å¡«è¯ç½®ä¿¡åº¦å¾ˆé«˜ï¼Œå¤§äº0.9864ï¼ˆé˜ˆå€¼å†è°ƒæ•´ï¼‰ï¼Œåˆ™ç›´æ¥ä½¿ç”¨ç»“æœï¼Œå¯å¤„ç†ä¸€äº›å¸¸è¯†å›ºå®šæ­é…é—®é¢˜
    å¦‚ï¼šå›½å®¶å[ä¹Œ]å…‹å…°ã€[ä¼Š]æ‹‰å…‹ç­‰
    :param sample_emoji: æµ‹è¯•é›†å•æ¡å«emojiçš„æ–‡æœ¬
    :param fill_mask: åŠ è½½é¢„è®­ç»ƒå‚æ•°çš„bertæ©ç å¡«è¯æ¨¡å‹
    :return: é«˜ç½®ä¿¡åº¦ç¿»è¯‘ç»“æœæˆ–è€…0
    """
    emoji_info = emoji.emoji_list(sample_emoji)
    if len(emoji_info) == 1:
        sentence = sample_emoji[:emoji_info[0]['match_start']] + '[MASK]' + sample_emoji[emoji_info[0]['match_end']:]
        out = fill_mask(sentence)[0]
        if out['score'] > 0.9864:
            # å»é™¤ä¸å¯èƒ½æ˜¯emojiç¿»è¯‘çš„å¹²æ‰°ç¿»è¯‘é¡¹
            if out['token_str'] in [',', '.', 'ï¼Œ', 'ã€‚', 'ï¼', 'ï¼Ÿ', 'ä½ ', 'æˆ‘', 'ä»–', 'å®ƒ', 'å¥¹', 'çš„', 'äºº', 'å§']:
                pass
            else:
                if out['token_str'] in ['äº†', 'æœ‰', 'æ ·', 'ä¸', 'ä¸ª', 'çœŸ', 'å› ', 'æ­¤', 'è°', 'æ¥', 'ä¸€', 'å¦‚', 'å¤š']:
                    pass
                else:
                    sentence = sample_emoji[:emoji_info[0]['match_start']] + out['token_str'] + \
                               sample_emoji[emoji_info[0]['match_end']:]
                    return sentence
    return 0


def generate_sentence_ppl(sentence, model, tokenizer):
    """
    åŸºäºé¢„è®­ç»ƒå‚æ•°è®¡ç®—å¥å­çš„å›°æƒ‘åº¦
    :param sentence: å¾…æ£€æµ‹å›°æƒ‘åº¦çš„å¥å­
    :param model: bertæ¨¡å‹
    :param tokenizer: bertçš„åˆ†è¯å™¨
    :return: å›°æƒ‘åº¦åˆ†æ•°
    """
    logging.set_verbosity_warning()
    with torch.no_grad():
        model.eval()

        tokenize_input = tokenizer.tokenize(sentence)
        tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])
        sen_len = len(tokenize_input)
        sentence_loss = 0.

        for i, word in enumerate(tokenize_input):
            # add mask to i-th character of the sentence
            tokenize_input[i] = '[MASK]'
            mask_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])

            output = model(mask_input)

            prediction_scores = output[0]
            softmax = nn.Softmax(dim=0)
            ps = softmax(prediction_scores[0, i]).log()
            word_loss = ps[tensor_input[0, i]]
            sentence_loss += word_loss.item()

            tokenize_input[i] = word
        ppl = np.exp(-sentence_loss / sen_len)

        return ppl


def creat_fill_mask(log=True):
    """
    ç›´æ¥åŠ è½½é¢„è®­ç»ƒçš„æ¨¡å‹å‚æ•°ï¼Œä¸éœ€è¦ç»§ç»­è®­ç»ƒ
    :return: å¯ç”¨äºæ©ç å¡«è¯çš„æ¨¡å‹
    """
    if log:
        print('--------------------generate_fill_mask--------------------')
    logging.set_verbosity_warning()
    logging.set_verbosity_error()
    model_path = "D:/dataset_all/bert_chinese/bert_base_chinese"
    fill_mask = pipeline(
        "fill-mask",
        model=model_path,
        tokenizer=model_path,
    )

    return fill_mask


def bert_handle_ride(sample_emoji, sentence, model, tokenizer):
    """
    ä½¿ç”¨bertå•å¥å›°æƒ‘åº¦åˆ¤æ–­ç‰¹å®šemojiå‰æ˜¯å¦éœ€è¦åŠ å†…å®¹
    :param sample_emoji: æµ‹è¯•é›†æ ·æœ¬
    :param sentence: å‰é¢ç¿»è¯‘å¥½çš„å¥å­
    :param model: bertæ¨¡å‹
    :param tokenizer: bertåˆ†è¯å™¨
    :return: å‘½ä¸­åˆ™è¿”å›ç¿»è¯‘å¥½çš„å¥å­ï¼Œå¦åˆ™è¿”å›åŸå¥å­
    """

    emoji_info = emoji.emoji_list(sample_emoji)
    if emoji_info[0]['emoji'] == 'ğŸ' or emoji_info[0]['emoji'] == 'ğŸš²':
        start_p = emoji_info[0]['match_start']
        best_sentence = sentence
        current_sentence = best_sentence[:start_p] + 'éª‘' + best_sentence[emoji_info[0]['match_start']:]
        score0 = generate_sentence_ppl(best_sentence, model, tokenizer)
        score1 = generate_sentence_ppl(current_sentence, model, tokenizer)
        if score0 > score1:
            return current_sentence
        else:
            return sentence
    else:
        return sentence


def bert_handle_verb(sample_emoji, sentence, model, tokenizer, skip_judge=0):
    """
    ä½¿ç”¨bertè¯­è¨€æ¨¡å‹å›°æƒ‘åº¦å¤„ç†åŠ¨è¯å†—ä½™é—®é¢˜
    :param skip_judge:
    :param sample_emoji: æµ‹è¯•é›†æ ·æœ¬
    :param sentence: ç¿»è¯‘åçš„æµ‹è¯•é›†æ ·æœ¬
    :param model: bertæ¨¡å‹
    :param tokenizer: bertåˆ†è¯å™¨
    :return: è¿›ä¸€æ­¥å¤„ç†çš„ç¿»è¯‘æˆ–è€…åŸå§‹ç¿»è¯‘
    """
    if len(emoji.emoji_list(sample_emoji)) == 1:
        if emoji.emoji_list(sample_emoji)[0]['match_start'] != 0:
            if sample_emoji[emoji.emoji_list(sample_emoji)[0]['match_start'] - 1] in ['åœ¨', 'å»', 'è¦'] or skip_judge:
                result = ''.join(list(difflib.Differ().compare(sample_emoji, sentence)))
                word = ''
                for i in range(len(result)):
                    if result[i] == '+':
                        word += result[i + 2]
                # need_add_verb.append([sample_emoji, word])
                if len(word) > 2 or skip_judge:
                    emoji_info = emoji.emoji_list(sample_emoji)

                    left = sample_emoji.split(emoji_info[0]['emoji'])[0]
                    right = sample_emoji.split(emoji_info[0]['emoji'])[1]
                    sentence0 = left + word + right
                    sentence1 = left + word[:-1] + right
                    score0 = generate_sentence_ppl(sentence0, model, tokenizer)
                    score1 = generate_sentence_ppl(sentence1, model, tokenizer)
                    # å¥å­æœ¬èº«å›°æƒ‘åº¦å°±å¾ˆå¤§å°±ä¸å¤„ç†äº†
                    if score0 < 500:
                        if score0 > score1:
                            return sentence1
                        if list(posseg.cut(word))[0].flag == 'n' and emoji.emoji_list(sample_emoji)[0]['emoji'] == 'ğŸ›’':
                            return sentence1
    # åªä¸å‘½ä¸­å°±è¿”å›åŸå¥å­
    return sentence


def bert_handle_adj(sample_emoji, sentence, model, tokenizer, data_adj, mode='off-line'):
    """
    ä½¿ç”¨bertè¯­è¨€æ¨¡å‹å›°æƒ‘åº¦å¤„ç†å¾ˆåŠ å½¢å®¹è¯é—®é¢˜
    :param sample_emoji: æµ‹è¯•é›†æ ·æœ¬
    :param sentence: ç¿»è¯‘åçš„æµ‹è¯•é›†æ ·æœ¬
    :param model: bertæ¨¡å‹
    :param tokenizer: bertåˆ†è¯å™¨
    :param data_adj: ç¦»çº¿æ¨¡å¼éœ€è¦ä½¿ç”¨çš„æ›¿æ¢è¯å…¸
    :param mode:å¤„ç†æ¨¡å¼ï¼Œoff-lineon-lineã€‚åœ¨çº¿ä¼šå¾ˆæ…¢ï¼Œç¦»çº¿æ˜¯å°†å¤„ç†ç»“æœä¿å­˜ä¸‹æ¥ç›´æ¥æ›¿æ¢
    :return: è¿›ä¸€æ­¥å¤„ç†çš„ç¿»è¯‘ç»“æœæˆ–è€…åŸå§‹ç¿»è¯‘ç»“æœ
    """
    if mode == 'off-line':
        for i in range(len(data_adj)):
            if data_adj[i][0].split('\t')[0] == sample_emoji:
                return data_adj[i][0].split('\t')[1]
        return sentence

    elif mode == 'on-line':
        emoji_candidate = generate_emoji_candidate()
        emoji_info = emoji.emoji_list(sample_emoji)
        if emoji_info[0]['match_start'] != 0:
            if sample_emoji[emoji_info[0]['match_start'] - 1] == 'å¾ˆ':
                jieba_cut = list(jieba.cut(sentence))
                if 'å¾ˆ' in jieba_cut:
                    if emoji_info[0]['emoji'] in emoji_candidate:
                        word_candidate = emoji_candidate[emoji_info[0]['emoji']]
                        # é™åˆ¶æœ€å¤šè€ƒè™‘å‰äº”çš„ç­”æ¡ˆ
                        word_candidate = word_candidate[:5] if len(word_candidate) > 5 else word_candidate

                        left = sample_emoji.split(emoji_info[0]['emoji'])[0]
                        right = sample_emoji.split(emoji_info[0]['emoji'])[1]
                        score_list = []
                        sentence_list = []
                        for i in range(len(word_candidate)):
                            sentence = left + word_candidate[i] + right
                            sentence_list.append(sentence)
                            score_list.append(generate_sentence_ppl(sentence, model, tokenizer))

                        if min(score_list) < 21:
                            return sentence_list[score_list.index(min(score_list))]
        return sentence
    else:
        print('error.mode:off-line|on-line')
        return 0


def bert_choose_word(sample_emoji, model, tokenizer, emoji_candidate):
    emoji_info = emoji.emoji_list(sample_emoji)
    if len(emoji_info) == 1:
        if emoji_info[0]['emoji'] in emoji_candidate:
            word_candidate = emoji_candidate[emoji_info[0]['emoji']]
            if '' in word_candidate:
                word_candidate.remove('')
            if 30 < len(word_candidate) < 50:
                word_candidate = word_candidate[:10]
            elif 3 < len(word_candidate) <= 30 or len(word_candidate) >= 50:
                word_candidate = word_candidate[:3]
            else:
                pass
            score = []
            left = sample_emoji.split(emoji_info[0]['emoji'])[0]
            right = sample_emoji.split(emoji_info[0]['emoji'])[1]
            for i in range(len(word_candidate)):
                sentence = left + word_candidate[i] + right
                score.append(generate_sentence_ppl(sentence, model, tokenizer))
            return left + word_candidate[score.index(min(score))] + right
    return sample_emoji


def bert_handle_fixed_verb(sample_emoji, sentence, model, tokenizer, _id):
    word_candidate = ['æ‹‰', 'çœ‹', 'çˆ¬']
    emoji_info = emoji.emoji_list(sample_emoji)
    if len(emoji_info) == 1:
        if _id not in [10378, 10535, 10666, 10765, 10801, 11403]:
            if emoji_info[0]['emoji'] in ['ğŸ“•', 'â›°', 'ğŸ’©']:
                start_p = emoji_info[0]['match_start']
                score0 = generate_sentence_ppl(sentence, model, tokenizer)
                score = []
                best_sentence = sentence
                for i in range(len(word_candidate)):
                    current_sentence = best_sentence[:start_p] + word_candidate[i] + best_sentence[start_p:]
                    score.append(generate_sentence_ppl(current_sentence, model, tokenizer))
                if min(score) < score0:
                    return best_sentence[:start_p] + word_candidate[score.index(min(score))] + best_sentence[start_p:]
    return sentence


if __name__ == '__main__':
    model, tokenizer = initialize_bert()
    print(bert_handle_fixed_verb('å‘¨æ—¥ä¸€èµ·å»â›°', 'å‘¨æ—¥ä¸€èµ·å»å±±', model, tokenizer))